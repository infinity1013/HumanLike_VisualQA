DropOut-> dropping some neurons to avoid overfitting

Activation Function
1)sigmoid -> dl/dw lies between 0-0.25
2)tanh -> dl/dw lies between 0-1
3)Relu-> to avoid Vanishing Gradient Problem
4)Leaky Relu-> to allow model to learn(update weights during backpropagation) when input is zero

Optimizers
1)Gradient Descent Optimizer
2)Adam
3)Adagrad 




